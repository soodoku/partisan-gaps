\documentclass[12pt, letterpaper]{article}
\usepackage[titletoc,title]{appendix}
\usepackage{color}
\usepackage{booktabs}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{dark-red}{rgb}{0.75,0.10,0.10}
\definecolor{bluish}{rgb}{0.05,0.05,0.85}

\usepackage[margin=1in]{geometry}
\usepackage[linkcolor=blue,
			colorlinks=true,
			urlcolor=blue,
			pdfstartview={XYZ null null 1.00},
			pdfpagemode=UseNone,
			citecolor={bluish},
			pdftitle={partisan_gap}]{hyperref}

\usepackage[resetlabels,labeled]{multibib}
\newcites{SI}{SI References}
\usepackage{natbib}

\usepackage{float}

\usepackage{geometry}  % see geometry.pdf on how to lay out the page. There's lots.
\geometry{letterpaper} % This is 8.5x11 paper. Options are a4paper or a5paper or other...
\usepackage{graphicx}  % Handles inclusion of major graphics formats and allows use of
\usepackage{amsfonts,amssymb,amsbsy}
\usepackage{amsxtra}
\usepackage{verbatim}
\setcitestyle{round,semicolon,aysep={},yysep={;}}
\usepackage{setspace} % Permits line spacing control. Options are:
%\doublespacing
%\onehalfspace
\usepackage{sectsty}    % Permits control of section header styles
\usepackage{pdflscape}
\usepackage{fancyhdr}   % Permits header customization. See header section below.
\usepackage{url}        % Correctly formats URLs with the \url{} tag
\usepackage{fullpage}   %1-inch margins
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{rotating}
\setlength{\parindent}{3em}

\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}

\usepackage{chngcntr}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{dcolumn}

\usepackage[nameinlink, capitalize, noabbrev]{cleveref}

\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}

\makeatother

\usepackage{footmisc}
\setlength{\footnotesep}{\baselineskip}
\makeatother
\renewcommand{\footnotelayout}{\normalsize \doublespacing}


% Caption
\usepackage[hang, font=small,skip=0pt, labelfont={bf}]{caption}
%\captionsetup[subtable]{font=small,skip=0pt}
\usepackage{subcaption}

% tt font issues
% \renewcommand*{\ttdefault}{qcr}
\renewcommand{\ttdefault}{pcr}


\setcounter{page}{0}

\usepackage{lscape}
\renewcommand{\textfraction}{0}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.40}
\setcounter{totalnumber}{5}
\makeatletter
\providecommand\phantomcaption{\caption@refstepcounter\@captype}
\makeatother

\title{A Measurement Gap? Effect of the Survey Instrument on the Partisan Knowledge Gap}

\author{Lucas Shen\thanks{Research Fellow at Asia Competitiveness Institute, Lee Kuan Yew School of Public Policy, at the National University of Singapore, \href{lucas@lucasshen.com}{lucas@lucasshen.com}},
  Gaurav Sood\thanks{Independent researcher, \href{gsood07@gmail.com}{gsood07@gmail.com}}, and
  Daniel Weitzel\thanks{Assistant Professor, Colorado State University, \href{mailto:daniel.weitzel@colostate.edu}{daniel.weitzel@colostate.edu}}}


\date{\today \thanks{Working paper, most recent version available at: \href{https://github.com/soodoku/partisan-gaps}{https://github.com/soodoku/partisan-gaps}}}

\begin{comment}

setwd(paste0(githubdir, "partisan-gaps/ms/"))
tools::texi2dvi("partisan_gap.tex", pdf = TRUE, clean = TRUE)
setwd(githubdir)

\end{comment}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}

\noindent Conventional wisdom suggests large, persistent gaps between partisans' stores of political knowledge, fanning concerns about democratic accountability. We reconsider the frequency and size of these ``partisan knowledge gaps,'' in  series of experiments. Manipulating frequently used survey items we demonstrate that survey design can inflate the partisan gap by 71\%. Our findings suggest that knowledge gaps---when they do exist---stem more from motivated responding than genuine differences in factual knowledge.

\end{abstract}

\vspace{.2in}

%{\bf Keywords:} Knowledge; Partisan Gap; Motivated Skepticism

\newpage

\doublespacing

The idea of a well-functioning representative democracy rests on the assumption of a more or less informed citizenry holding its representatives and political parties accountable for the successes and failures in office \citep{schattschneider-1960}. Empirical examinations of this assumptions initially focused on the prevalence (or absence) of knowledge \citep{dellicarpini} and then moved to the effect of the ``perceptual screen'' \citep[p. 133]{campbell1980american} of partisanship on the type of things people know \citep{bartels_2002}. Partisan divisions raise concerns about the funtioning of the above mentioned process of democratic accountability \citep{hochschild2015isn}. In particular, research showing large differences in what partisans believe to be true about politically consequential things has cast a shadow on the prospect of democracy \citep{campbell1980american, jerit2012partisan}. If partisanship is in fact the pervasive force that perpetuates and reinforces how partisans see and understand the political world \citep[p. 138]{bartels_2002} serious normative implications about the functioning of democracy arise. A new line of research, however, suggests that a large fraction of the observed partisan difference in beliefs is an artifact of the survey response process \citep{bullocketal_2015,huber_yair_2018, prior2015you}. For instance, \cite{bullocketal_2015} find that nearly half the partisan gap in political knowledge is not a result of differences in beliefs but a result of expressive responding or partisan inference.

In this paper, we extend the investigation into the role of survey design in explaining partisan differences. We propose two theories of partisan knowledge gaps. The first argues that partisan gaps reflect actual differences in beliefs about the world. The second proposes that partisan gaps are, to a large part, artifacts of survey and questionnaire design. We report results from a new set of experiments that manipulate common features of frequently used survey items. The features we focus on plausibly encourage people to guess when they don't know or report attitudes instead of knowledge, and thereby encourage partisan inference causing inflated partisan gaps in knowledge. We also assess what difference items that not only assess knowledge but also confidence in that knowledge make on the prevelance and size of partisan gaps. After all, knowledge is a confidently held correct belief about something. Do the partisan gaps persist if only survey responses that participants are confident in are coded as correct?
%more stringent way to code the answers---only coding responses that respondents are confident about as correct---makes to observed differences.

% 52% = Table 2 RepXFSR
% 72% = Table 4 column 1
% 43% = Table 3
% 71% = Table 2 colum 3
Across two experiments and thirteen items, we find that about 52\% of the knowledge differences between partisans are due to survey measures that encourage respondents to guess when they don't know. Across two other experiments, we find that survey features that encourage partisan inferences inflate the observed differences by up to 72\%. Lastly, we find that a coding scheme that only codes answers that respondents are confident in reduces partisan gaps by 43\% to 72\%. Our findings support the second theory of partisan knowledge gap formation. Current designs of survey items can encourage participants to not report confidently held knowledge but use partisan cues to express attitudes and opinions about the world. In contrast to incentive-based experimental designs that use (monetary) rewards to encourage respondents to overcome their perceptual screen of partisanship when it comes to answering knowledge questions \citep[for example, see][]{bullocketal_2015, peterson2021partisan} our research design examines the effect that question design has on increasing the partisan gaps in knowledge. Our findings offer practical advice on how to, with no additional costs, decrease the gap through simple and senseable adjustments to the questionnaire design.

\newpage

\section*{Two Theories of Partisan Gaps}

Research has repeatedly shown that partisan gaps in political knowledge are wide and widespread \citep{bartels_2002, jerit2012partisan, lodgetaber_2013}. For instance, when Americans were quizzed at the end of Bill Clinton's first term in 1996 about whether the budget deficits increased, decreased, or remained the same, 39\% of Democrats correctly identified that the budget deficit had decreased, only 25\% of Republicans did the same \citep[280]{achen2016democracy}.
% TODO: Should we add a more recent example here?

%slothuus2021political

There are two broad explanations for these gaps: The first is that partisan gaps on partisan consequential knowledge and misinformation items are a result of the fact that partisans know different things. The second theory is that partisans gaps are an artifact of the survey design.


% THIS IS THE SAME AS ABOVE
%\subsection*{Partisan Gaps in Knowledge}
%Research shows that partisan gaps in political knowledge are wide and widespread \citep{bartels_2002, jerit2012partisan, lodgetaber_2013}. For instance, when Americans were quizzed at the end of Bill Clinton's first term about whether the budget deficits increased, decreased, or remained the same during the last four years, while 39\% of Democrats correctly identified that the budget deficit had decreased, only 25\% of Republicans did the same \citep[280]{achen2016democracy}.

%There are two broad explanations for these gaps. The first is that partisan gaps on partisan consequential knowledge and misinformation items are a result of partisans knowing different things. The second theory is that partisan gaps are an artifact of the survey interview process.

\subsection*{Partisan Differences in Beliefs}
Partisan gaps in survey measures of political knowledge and misinformation may reflect \emph{actual differences} in what partisans believe to be true. These differences in beliefs may, in turn, stem from selective exposure to information or motivated reasoning. Selective exposure to information---partisans being exposed to more congenial than uncongenial information--can affect what facts people know about the world \citep{stroud_2010}. Even without partisans favoring congenial information, it could be that because of the kinds of people that partisans are, certain information is more readily available. As an example, African Americans, who overwhelmingly identify as Democrats, may be more exposed to negative consequences of economic downturns and may hence have different beliefs about economic conditions than Caucasians, a majority of whom identify as Republicans. Similarly, selective exposure may stem from different 'tastes' in politics. For instance, partisans of different stripes may be interested in different policies, politicians, etc. Taken thus, the partisan gap might be similar to other types of knowledge gaps across groups---see research on gaps in gender \citep{dolan2011women, barabas2014question} and race \citep{abrajano2015reexamining}. Conventionally, however, partisan gaps are thought to stem from information avoidance---people find information that is dissonant to their worldview to be painful and work to avoid it  \citep[e.g.,][]{abelson1959modes,festinger1962theory}.

Whatever the cause, the effect of selective exposure is undoubtedly made worse by ``motivated skepticism'' \citep{taber2006, stroud2008media}. People are more skeptical of uncongenial than congenial information \citep{Zaller1992}. This can have various consequences on how individuals engage with congenial and uncongenial information they might encounter. It is possible that citizens are more likely to follow up and do the due diligence to disprove uncongenial information. They may also simply be more likely to distrust and ignore uncongenial information. And lastly, even when people are receiving congenial and uncongenial informationa at the same rate, they may be less likely to actually remember uncongenial information \citep[see, for example][]{bayes2020and,hill2017learning,flynn2017nature, taber2006}. Recently,  \citet{peterson2021partisan} have, for example, shown that political polarization increases the readiness of individuals to accept information that corroborates ideological or partiasan beliefs and vice versa disregard or challenge facts that run counter to them.

To summarize, it is possible that due to selective exposure or motivated skepticism the observed partisan gaps in political knowledge in survey research reflect actually existing differences in beliefs about what is true about the world.

\subsection*{Artifact of Survey Design}
Partisan gaps on partisan consequential knowledge and misinformation items in surveys may be an \emph{artifact of questionnaire design}.

In aggregate, the answers to survey questions about factual beliefs reflect a mixture of knowledge, inferences, cheating, expressive responding, and guesses by the respondents. Inferences, cheating, and guessing cause structured error in our estimates, by inflating our estimates of how many people believe something to be true. These three ways of answering survey questions also affect our estimates of partisan gaps in beliefs. Primarily, inferences with a partisan tint and expressive responding--responding to questions about beliefs to indicate partisan positions--inflate the estimates. On partisan consequential items---items where the right answer has implications about how good the party looks---inferences with a partisan tint are likely common. For instance, when partisans don't know the answer to a question, they likely use affect as a guide to infer the answer \citep{malka2022expressive}. As an example, when asked about what happened to the federal deficit during the Obama administration, Republicans, thinking Democrats cause bad things, may infer that deficits increased under Obama. Alternately, partisans may rely on stereotypical inference. Republicans may think of Democrats as generally indifferent to deficits, and hence may infer, without actually knowing, that it increased under Mr. Obama \citep[e.g.][]{rahn1993role, goggin2020goes}. In a highly polarized political environment minimal information can be enough to switch individuals from answering a knowledge question to using affect or expressive motiavtions to answer a questions \citep{klar2014partisanship, merkley2018party}.

The extent to which survey responses are contaminated by responses other than strongly held beliefs is conditional on survey features. Surveys can encourage respondents to respond 'expressively' by highlighting partisan motivations over accuracy motivations \citep{Zaller1992}. This explanation has attracted considerable research. Some of it shows that up to half of the partisan gaps are a result of expressive responding \citep[][though see \citet{berinsky_2017}]{bullocketal_2015,huber_yair_2018, prior2015you}.




\subsection*{Empirical Implications of the Theories}

If partisan gaps are a result of actual differences, minor differences in question wording and response options stem should principally have little effect on the gap. On the other hand, if the gaps are sensitive to question and response attributes, it suggests that some of the partisan gaps may not be founded in differences in beliefs. In particular, we contend that political surveys regularly include features that inflate partisan gaps to produce sensational results.
% TODO: that is a bold statement, are we sure that we want to include sth like that?

Surveys regularly exclude don't know \citep{luskin2011don}, include guessing encouraging features such as providing background information and social proof in the stem that likely makes people think that they know something about the topic \textbf{(TODO CITE)} or give them extra information that they can use to guess the answer \textbf{(TODO CITE)}. Often enough, surveys also include partisan cues \textbf{(TODO CITE)}. And the scoring rules used by analysts don't disambiguate between respondents who are confident about their answers and those who aren't \textbf{(TODO CITE)}. We suggest that removing these inflationary features diminishes the partisan gaps in political knowledge.

To test the conditionality of partisan knowledge gaps in survey data we fielded four surveys that test the effect different aspects of survey and question design can have on (partisan) response patterns. In studies 1 and 2, we used Amazon Mechanical Turk (MTurk) to ask participants a variety of knowledge questions in different designs. These items aim at examining how survey instructions, question wording, response options, and response design in the survey affect partisans to respond to questions in specific ways. In studies 3 and 4, we examine the role of question wording on response behavior in more detail by focusing on the effect that partisan-related auxilliary information can have on response patterns. We will first turn to the impact of inflationary survey design on knowledge gaps (studies 1 and 2) before examining partisan-related cues (studies 3 and 4).

\newpage

\section*{The Effect of Inflationary Survey Design}
\label{sec:inflationary_measures}
%\section*{Impact of Don't Know, Neutral Information in the Question Stem, and Scoring Rules}

Studies 1 and 2 focus on three survey design features that we suspect might inflate the partisan gap in political knowledge. These features are the absence of a ``Don't Know'' option%\footnote{Notably, \citet{peterson2021partisan} don't offer their respondents the option to indicate that they don't know the correct answer to a question and thereby encourage guessing.}
, including partisan-related information in the question stem, and not differentiating between confidently and weakly held beliefs about facts.

\subsection*{Data and Research Design}\label{sec:data}

Both surveys were fielded on Amazon's Mechanical Turk \citep{BerinskyHuberLenz2012} in the second quarter of 2017. In the survey for Study 1, we randomly assigned 1,253 respondents to one of five conditions with varying experimental treaments testing the effect of inflationary components of survey questions.

In each condition respondents answered 9 misinformation items, ranging from citizenship and religion of Obama to whether global warming is happening or not.\footnote{The exact question wording for each of the items is presented in \cref{si:mturk}.} Respondents assigned to the first two conditions (inflationary and real-world design) saw a simple preface: ``Now here are some questions about what you may know about politics and public affairs,'' while in all the other conditions, they were reassured that it is ok to not know answers to these questions and to commit to not looking up answers or asking anyone and to mark don't know when they, as research has shown is frequently the case, in fact don’t know the correct answer to a question.\footnote{Again, see \cref{si:mturk} for the specific wording.}

In the survey for Study 2, we randomly assigned 1,059 respondents to either closed-ended or confidence coding questions of four items. The topics and answer options of these questions were indentical to Study 1 and included questions about the Affordable Care Act (2), the effect of greenhouse gases (1), and the consequences of then-president Trump's executive order on immigration(1). In the multiple choice version of the survey the participants received five options as answers, including a ``Don't Know'' option. In the confidence coding version of the survey respondents were asked to report the confidence with which they knew an item to be correct or incorrect. In this treatment the individuals were asked the same questions as in the multiple choice treatment and they had to report the confidence with which they considered each of the four statements that were answer options in the multiple choice questions to be correct.\footnote{The exact question wording for each of the items is presented in \cref{si:mturk2}.}


\begin{table}[]
\centering
\caption{Features of Media Poll Misinformation Items}
\label{tab:features}
\begin{tabular}{lc}
\hline
\textbf{Design Feature} & \textbf{Proportion of Items} \\ \hline
Explicit DK/Not Sure    &                              \\
DK                      & .067                         \\
Not Sure                & .022                         \\
                        &                              \\
SR-Encouraging wording  &                              \\
TBYK                    & .167                         \\
DYT                     & .400                         \\
AFYK                    & .117                         \\
Other                   & .318                         \\
                        &                              \\
Number of options       &                              \\
2                       & .417                         \\
3                       & .172                         \\
4                       & .139                         \\
5 or more               & .161                         \\
                        &                              \\
IRE                     & .061                         \\ \hline
\multicolumn{2}{l}{\textit{Notes: n = 180}}
\end{tabular}
\end{table}

% TODO: How exactly are things different here? In the original text there was no difference between Real World (RW), Iron Pyrite Standard (IP),
% TODO 1: better labels for conditions
% TODO 2: if table is okay for doc add as own .tex to tabs folder

\input{../tabs/treatment_labels.tex}

In total this yields five conditions, four multiple choice and one Likert scale design, that successively remove inflationary survey design features, as shown in \Cref{tab:conditions}. Items can include a `Don't Know' option, offer social proof of the incorrect answer (such as ``some people belief that Barack Obama was not born in the U.S.''), have neutral information that encourages people to guess, explicitly encourage guessing, and ask people about the confidence with which they know something. Each condition is explained in detail below:


\begin{itemize}

  \item Condition 1: The \textit{Inflationary Design Approach} (IDA), in this version we replicate design features from highly partisan surveys that do not have the goal of collecting representative opinion data but push an agenda. In this design, `Don't Know' options are never presented and respondents can't indicate lack of knowledge. These questions also include social proof  about the incorrect answer, for instance, ``Some people believe Barack Obama was not born in the United States, but was born in another country'' on a question about where Mr. Obama was born, and some neutral information about the topic, like ``According to the Constitution, American presidents must be `natural born citizens''' on the birthplace question, that may encourage the ignorant to take a guess.  This condition does not score the confidence with which knowledge is held.

  \item Condition 2: The \textit{Commonly Used Design} (CUD), reflects the real-world standards in (nonacademic) polling most closely. These questions are very similar to the IDA questions but usually do not include social proof. In our experiments, these questions do not feature a `Don't Know' option, include neutral information in the question stem, encourage guessing, and do not ask respondents to report how confidently they hold the knowledge.

  \item Condition 3: The \textit{Fewer Substantive Responses} (FSR) design likely reduces the number of substantive responses to survey questions by including a `Don't Know' option and thereby offering participants the option to reveal ignorance. In doing so, respondents are not forced to pick a substantive answer category when they don't have an opinion. Since research has repeatedly shown how prevelant the absence of political knowledge is we consider this an important feature for designing non-inflationary partisan knowledge surveys. These questions do not provide social poof but have encourage guessing and have a neutral question stem that might provide people information to base that guess on.

	\item Condition 4: The \textit{Improved Multiple Choice} condition is the best version of multiple choice questions. It offers individuals a `Don't Know' option, does not include any social proof, and does not encourage guessing. These changes to question formulation and design have been done while maintaining commensurability with other items. This approach minimizes inflationary features in questions with minimal changes to questionnaire and survey design.

	\item Condition 5: The \textit{Confidence Coding Design} (CCD) condition focuses on the confidence scoring of knowledge. Respondents rate a series of claims on a 0 to 10 scale going from `definitely false' to `definitely true.' The question is inspired by other attempts to take account of confidence in distinguishing misinformation from incorrect responses stemming from processes like inference, unlucky guessing, and such (see, for instance, \citet{pasek2015}). While we consider this to be the gold standard when it comes to removing inflationary features from the survey design it is a larger deviation from common survey design features. This question design does not encourage guessing and features no social proof.

\end{itemize}

\subsection*{Results}
We test the effect of these five different conditions on the partisan gaps in political knowledge and present results about gradually removing inflationary features from the questions.


\subsubsection*{Study 1}
We start by summarizing the average partisan gap for each survey item and each treatment arm from the MTurk sample of Study 1 in \Cref{fig:partisangaps-mturk}. Each marker represents how much more congenial the responses of the Republicans are to the Democrats. In the Condition 1 treatment arm (first column), the Republicans are, on average, 30 percentage points more likely than the Democrats to have party-congenial responses. The subsequent four columns in \cref{fig:partisangaps-mturk} show that, while the estimated differences in party-congenial responses are precise (the narrow bars), the differences attenuated substantially depending on the treatment arms. Removing more and more inflationary features from the questions decreases the partisan gap in political knowledge.

% Table for the unstandardized partisan gap estimates from study 1
% results are presented in the partisan-gap-by-item-arm.pdf figure
%\input{../tabs/unstandardized-pg-mturk.tex}

\begin{center}
	\begin{figure}[t]
		\centering
		\caption{Partisan Gap by Treatment Arm (MTurk)}
		\includegraphics[width=\textwidth]{../figs/partisan-gap-by-item-arm.pdf}
		\label{fig:partisangaps-mturk}
		\caption*{\footnotesize 
			Each marker is the estimated difference in proportions for how congenial their responses are to their own party.
			Columns indicate the five different conditions described in \nameref{sec:data}. Rows indicate the nine individual survey question items described in \cref{si:mturk} plus their average.
			Each point is the estimated $\beta$ from estimating $1\{party\text{-}congenial\, response\}_i = \alpha + \beta Rep_i + \varepsilon_i$ for each of items and each of the five arms.			
			Horizontal bars are 95\% confidence intervals constructed from robust standard errors.
		}
	\end{figure}
\end{center}

The attenuation is most pronounced when comparing Condition 1 to Condition 5 (first vs. last columns). In the Condition 5 arm, Republicans are, on average, only about 10 percentage points more likely to have party-congenial responses, a drop larger than 50 percent. \Cref{fig:partisangaps-mturk} therefore gives us the first indication that partisan gaps arise, at least in part, from questionnaire artifacts present in the different survey arms.

We formalize the above observation as follows. We regress the dependent variable, an indicator of whether the response is party-congenial, on the interaction of partisanship and the treatment arm:
\begin{equation}\label{eq:partisangap-mturk}
y_{ijk} = \alpha + \beta (Rep)_i + \gamma (Arm)_k + \delta_k (Rep_i \times Arm_k) + (survey \; item)_j + \varepsilon_{ijk}
\end{equation}
for respondent $i$, survey item $j$, and treatment arm $k$. $\beta$ is the difference in partisan knowledge gaps, corresponding to the markers in \cref{fig:partisangaps-mturk}. A positive estimate suggests that Republicans are more likely than Democrats to have a party-congenial response. We focus on the $\delta$'s, which capture how the different treatment arms affect observed partisan knowledge gaps (difference between columns in \cref{fig:partisangaps-mturk}). The baseline treatment arm is always RW, so $\delta$ captures how the four treatment arms---having the same questions with different questionnaire artifacts ---mediates partisan knowledge gaps.
We include the survey item fixed effects to allow each item to elicit some constant amount of partisan gap, if any, from the respondents. Standard errors are clustered at the respondent level.

\begin{figure}[t]
	\centering
	\caption{Partisan Gap by Treatment Arm: MTurk}
	\includegraphics[width=.55\textwidth]{../figs/mturk-pgag-surveyarms.pdf}
	\label{fig:partisangaps-mturk-reg}
	\caption*{\footnotesize 
		Difference between bars indicates the predicted partisan gap by the treatment arms. 
		Bars reconstructed from the interactions of the Republican indicator with the treatment arms as reported in column (3) of \cref{tab:partisangaps-mturk}.
		The baseline arm is RW.
		Capped vertical bars are 95\% confidence intervals.
	}
\end{figure}


\begin{table}[t] \centering \small \setlength\tabcolsep{0 pt} \setlength{\defaultaddspace}{0pt}
	\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
	\caption{Partisan Knowledge Gaps: MTurk}
	\label{tab:partisangaps-mturk}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{l*{6}{D{.}{.}{-1}}}
			\toprule
			% https://tex.stackexchange.com/questions/567985/problems-with-inputtable-tex-hline-after-2020-fall-latex-release
			\input ../tabs/mturk-reg-table-fragment.tex
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption*{\footnotesize All models are linear probability models where the dependent variable indicates whether the response to a survey item is congenial to party affiliation. Demographic controls include age cohort, gender, education level (college degree, high school, no high school, post-graduate, and some college), and race (Hispanic, Asian, Black, White, Others). All models include the nine survey item fixed effects. Standard errors are clustered at the respondent level. Significance levels: + 0.1 * 0.05 ** 0.01 *** 0.001.}
\end{table}


\cref{tab:partisangaps-mturk} reports the results from estimating \cref{eq:partisangap-mturk}. Column (1) includes just the Republican variable, which is significant and consistent with conventional wisdom about gaps in partisan knowledge \citep[e.g.][]{bullocketal_2015, pew2018disagree}.
Column (2) includes only the treatment arms, and three of them elicit differences in partisan gaps that are statistically different from the baseline RW arm. While the treatment arm estimates are not as large as the Republican variable in column (1), it is still substantial evidence of how variable the estimated knowledge gap can be in the presence of questionnaire artifacts.

Moreover, it is variable in a way that is independent of partisanship. Without accounting for partisanship, for instance, the average respondent assigned to the 24k arm is 17 percentage points less likely to give a party-congenial response than the RW arm ($p<0.001$). This estimate is non-trivial.
Compared to the gap arising from partisanship (in column (1)), a survey artifact in terms of differences in survey features (in column (2)) elicits a gap two-thirds as large.

In column (3) of \cref{tab:partisangaps-mturk}, we include the interaction of partisanship and treatment arms. Now the Republican variable captures the partisan gap in the RW arm (corresponding to column (1) of \cref{fig:baltest-24k-rw}). The Republican and treatment arms interactions reveal the extent to which partisan knowledge gaps change across the different treatment arms. 

\cref{fig:partisangaps-mturk-reg} shows the estimates in absolute terms. For the FSR interaction term, just adding a `Don't Know' response option reduces the estimated partisan knowledge gap by more than half ($p<0.001$).
The largest reduction is 71 percent ($p<0.001$), which comes from the 24k arm. This arm allows respondents to rate their responses on a 0 to 10 scale from `definitely false' to `definitely true' instead of a false and true option, where only a response with confidence of 10 is considered. In columns (4)--(6), including the self-reported characteristics of respondents does not change the conclusion. Overall, the MTurk sample of Study 1 reveals that measured partisan knowledge gaps are highly sensitive to different questionnaire artifacts in the same questions.


\subsubsection*{Study 2}
% TODO: What to do with this stuff?
\begin{center}
	\begin{figure}[h]
		\centering
		\caption{Partisan Gaps in Knowledge in different question designs}
		\includegraphics[width=.55\textwidth]{../figs/mturk-hk-MC-LIKERT.pdf}
		\label{fig:mturk_hk}
		\caption*{\scriptsize 
			Figure shows the estimated partisan gaps in knowledge from the MTurk sample for Study 2 for two different survey conditions.
			The multiple choice condition provides five closed-ended options, including the correct answer and a ``Don't Know''.
			The Likert scale condition only considers the selection of the correct answer with a full confidence of 10 (see \cref{si:mturk2}). 
			Estimates correspond to those reported in \cref{tab:mturk_hk}.
			Horizontal bars are 95\% confidence intervals.
		}
	\end{figure}
\end{center}

\input{../tabs/mturk_hk_mc_likert.tex}

\newpage

\section*{The Effect of Partisan Cues on Partisan Gaps}\label{subsec:partisan-cues}
The aim of the study is to present experimental evidence about effect of partisan cues in the question stem on responses by partisans. For the purpose of the study, we examine closed-ended items asking about policy-relevant facts or objective performance, particularly those items stirring affective consistency, stereotyping, or both.  In the first case, items whose correct response option one side or the other would like to disbelieve, or at least one of whose incorrect response options one side or the other would like to believe, or both; in the second case items whose correct response option defies stereotype, or at least one of whose incorrect response options conforms to stereotype, or both.  

For exploring the research question, we exploit two datasets---a national survey conducted by YouGov, and a telephone survey of a random sample of adults in Texas. The YouGov survey interviewed 2000 respondents between July 10th and 12th, 2012.  In Texas, a total of 1003 interviews were conducted between September 10th and 21st, 2012. 

In the YouGov survey, respondents were randomly assigned to factual questions with either a Republican or Democratic cue in the stem. In a question about whether ``since 2010 midterm elections, the unemployment rate [had] gone up, down, or remained the same, or couldn't you say?'', we inserted either the phrase “when Republicans regained control of the U.S. Congress'' or ``when Democrats retained control of the Senate” right after the first phrase. We employed a similar manipulation for the question on budget deficit, asking how the budget deficit had fared “since the 2010 midterm elections, when Republicans regained control of the U.S. Congress (or ``when Democrats retained control of the Senate''), has the budget deficit gone up, gone down, remained the same, or couldn't you say?''

In the Texas survey, we added another condition to the above design – no partisan cue in the stem. So a third of the respondents were assigned to a question that simply read, ``since the 2010 midterm elections, has the unemployment rate gone up, gone down, or remained the same?  Or couldn’t you say?'' For the second question we changed our design to – no partisan cue, Democratic cue, and Democratic cue plus the following introduction “based on what you have heard”. The question read, ``since January 2009, have federal taxes increased, decreased, or remained the same or couldn’t you say?.'' The second version gave respondents a Democratic cue by changing the initial part of the sentence; the question now read, “Since Barack Obama took office\ldots''  The third version prepended a cue designed to encourage guessing to the second version; the version read, “Based on what you have heard, since Barack Obama took office, \ldots''


\subsection*{Results: Partisan Knowledge Gaps with Partisan Cues (YouGov)}
\begin{figure}[t]
	\caption{Partisan Knowledge Gaps with Partisan Cues: YouGov Survey}	
	\centering
	\begin{subfigure}{.495\textwidth}\centering
		\includegraphics[width=\textwidth]{../figs/yougov-unemp-congenialcue.pdf}
		\caption{Unemployment}
	\end{subfigure}
	\hfil
	\begin{subfigure}{.495\textwidth}\centering
		\includegraphics[width=\textwidth]{../figs/yougov-deficit-congenialcue.pdf}
		\caption{Budget deficit}
	\end{subfigure}	
	\caption*{\footnotesize Bars indicate the predicted percent of responses saying that unemployment or the budget deficit have gone up (correct responses) as reported in \cref{tab:partisangaps-yougov} (columns (1) and (4)).  
		Capped vertical bars indicate 95\% confidence intervals.
	}
	\label{fig:yougov-reg}
\end{figure}

We start with the YouGov survey to provide experimental evidence that cues in survey questions can affect responses to questions about policy-relevant and objectively verifiable facts. This survey includes questions about changes in unemployment and the budget deficit since the 2010 midterm elections, with manipulated partisan cues in the stem. 

Using the YouGov survey responses, we estimate
\begin{equation}\label{eq:pgap-yougov}
\text{correct response}_{i} = \alpha + \beta (congenial \; cue)_i  +\varepsilon_{i},
\end{equation}
where the dependent variable is the indicator of whether the response to the question is correct.
As discussed above in \cref{subsec:partisan-cues}, we model the correct response rate as dependent on whether the cue presented to individuals is congenial to responding correctly. Specifically, the congenial cue indicator is coded as one when a Democrat receives a question stem with the cue ``when Republicans gained control of the US congress.'' This cue manipulates Democrats into blaming the Republicans by suggesting that unemployment has gone up, which is the correct response. The reverse happens for Republicans. The congenial cue for Republicans is coded as one when they receive the cue ``When Democrats retained control of the Senate.''

\begin{table}[t] \centering \normalsize \setlength\tabcolsep{0 pt} \setlength{\defaultaddspace}{0pt}
	\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
	\caption{Partisan Knowledge Gaps with Partisan Cues: YouGov}
	\label{tab:partisangaps-yougov}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{\hspace{0\tabcolsep}}l*{6}{D{.}{.}{-1}}@{\hspace{0\tabcolsep}}}
			\toprule
			% https://tex.stackexchange.com/questions/567985/problems-with-inputtable-tex-hline-after-2020-fall-latex-release
			&\multicolumn{3}{c}{Unemployment has gone up}&\multicolumn{3}{c}{Deficit has gone up}\\
			\cmidrule(lr){2-4}\cmidrule(l){5-7} 
			\input ../tabs/yougov-reg-table-fragment.tex
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption*{\footnotesize Dependent variables indicate whether the individual responded that unemployment or the budget deficit had gone up since the 2010 midterm elections (which are the correct responses).
		Congenial cue indicates whether the question stem includes the cue towards getting the correct response. For Democrats, this is when the question stem includes the cue ``when Republicans gained control of the US Congress.''
		For Republicans, this is when the question stem includes the cue ``when Democrats retained control of the Senate.''
		Demographic controls include age cohort, gender, education level, marital status, employment status, news interest, family income, and race. Standard errors are heteroskedasticity-robust. 
		All models are linear probability models. 
		Significance levels: + 0.1 * 0.05 ** 0.01 *** 0.001.}
\end{table}
Panel (a) of \cref{fig:yougov-reg} shows that, by manipulating the partisan cue that respondents receive, the probability of getting the correct response for the unemployment question differs by 14 percentage points ($p < 0.001$, reported in \cref{tab:partisangaps-yougov}).

Panel (b) of \cref{fig:yougov-reg} shows that this systematic difference is not unique to the unemployment question. We reestimate \cref{eq:pgap-yougov} where the dependent variable is getting the correct response that the budget deficit has gone up. When the individuals get a congenial cue, they are 18 percentage points more likely to get the correct response ($p<0.001$).
Presumably, we observe this congenial cue effect because the question stem holds the other party responsible for the increase in unemployment and deficit, which are both undesirable.\footnote{\cref{fig:yougov-reg-by-partisanship} show that there is some heterogeneity in how the congenial cue affects Republicans as opposed to Democrats. However, the effect is not unique to either party since partisans of both types are more likely to get the correct response when randomly assigned the congenial cue.}

\subsection*{Results: Partisan Knowledge Gaps with Partisan Cues (Texas Lyceum)}
\begin{figure}[!t]
	\centering
	\caption{Partisan Gap by Treatment Arm: Texas Lyceum, Unemployment}
	\includegraphics[width=.55\textwidth]{../figs/texas-unemp-congenialcue.pdf}
	\label{fig:partisangaps-texas-unemp}
	\caption*{\footnotesize 
		Bars indicate the predicted percent of responses saying that unemployment has gone up (correct response) as reported in column (1) of \cref{tab:partisangaps-texas-unemp}.  
		Capped vertical bars indicate 95\% confidence intervals.
	}
\end{figure}

We further supplement our results with the Texas Lyceum survey, which includes a third cue: a neutral cue. For the question about unemployment in this survey, in addition to congenial and uncongenial cues, individuals can also be randomly assigned a neutral cue where the additional question stem assigning blame to a party is absent, giving us a total of three groups: (i) no cue, (ii) congenial cue, and (iii) uncongenial cue.

\cref{fig:partisangaps-texas-unemp} shows that our results above still hold when we include a neutral cue. Compared to individuals who received a neutral cue, individuals who receive an uncongenial cue are 17 percentage points less likely to get the correct answer that unemployment has gone up ($p<0.001$). Individuals who receive a congenial cue are 8 percentage points more likely to get the correct answer ($p<0.1$). These results are tabulated in \cref{tab:partisangaps-texas-unemp}.

\begin{table}[t] \centering \normalsize \setlength\tabcolsep{6 pt} \setlength{\defaultaddspace}{0pt}
	\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
	\caption{Partisan Knowledge Gaps with Partisan Cues: Texas Lyceum, Unemployment}
	\label{tab:partisangaps-texas-unemp}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{\hspace{0\tabcolsep}}l*{3}{D{.}{.}{-1}}@{\hspace{0\tabcolsep}}}
			\toprule
			% https://tex.stackexchange.com/questions/567985/problems-with-inputtable-tex-hline-after-2020-fall-latex-release
			&\multicolumn{3}{c}{Unemployment has gone up}\\
			\cmidrule(l){2-4}
			\input ../tabs/texas-unemp-reg-table-fragment.tex
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption*{\footnotesize Dependent variable indicates whether the individual responded that unemployment has gone up since the 2010 midterm elections (which is the correct response).
		Congenial cue indicates whether the question stem includes the cue towards getting the correct response. 
		For Democrats, this is when the question stem includes the cue ``when Republicans regained control of the US Congress.''
		For Republicans, this is when the question stem includes the cue ``when the Democrats retained control of the Senate.''
		Demographic controls include age cohort, gender, education level, marital status, number of children, children school enrollment, family income, religion, liberalism/conservatism, and race. Standard errors are heteroskedasticity-robust. 
		All models are linear probability models. 
		Significance levels: + 0.1 * 0.05 ** 0.01 *** 0.001.}
\end{table}


\begin{table}[t] \centering \normalsize \setlength\tabcolsep{6 pt} \setlength{\defaultaddspace}{0pt}
	\def\sym#1{\ifmmode^{#1}\else\(^{#1}\)\fi}
	\caption{Partisan Knowledge Gaps with Partisan Cues: Texas Lyceum, Federal Taxes}
	\label{tab:partisangaps-texas-fedtax}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{\hspace{0\tabcolsep}}l*{4}{D{.}{.}{-1}}@{\hspace{0\tabcolsep}}}
			\toprule
			% https://tex.stackexchange.com/questions/567985/problems-with-inputtable-tex-hline-after-2020-fall-latex-release
			&\multicolumn{2}{c}{Responded ``Gone up''}&\multicolumn{2}{c}{Responded ``Don't Know''}\\
			\cmidrule(lr){2-3}\cmidrule(l){4-5} 
			\input ../tabs/texas-fedtax-reg-table-fragment.tex
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption*{\footnotesize Dependent variables indicate whether the individual responded that federal taxes had gone up since the 2010 midterm elections (which are the correct responses) or ``don't know''.
		Congenial cue indicates whether the question stem includes the cue towards getting the correct response. 
		Only Republicans can get a congenial cue for these questions.
		This happens when Republicans receive the question stem that includes the cue ``since Barack Obama took office.''
		Separately, individuals can also be assigned a cue that encourages guessing. This happens when the question stem includes ``Based on what you have heard, since Barack Obama took office...''
		Demographic controls include age cohort, gender, education level, marital status, number of children, children school enrollment, family income, religion, liberalism/conservatism, and race. Standard errors are heteroskedasticity-robust. 
		All models are linear probability models. 
		Significance levels: + 0.1 * 0.05 ** 0.01 *** 0.001.}
\end{table}


Finally, we examine the federal taxes question in the Texas Lyceum survey, where individuals are asked whether federal taxes have increased, decreased, or remained the same. For this question, individuals are randomly assigned (i) the Democratic cue ``Since Barack Obama took office'', (ii) the Democratic cue with an additional cue that encourages guessing ``Based on what you have heard, since Barack Obama took office...'', and (iii) a neutral stem.

Based on the estimates in \cref{tab:partisangaps-texas-fedtax}, we observe that randomly receiving a congenial cue still leads to a higher correct response rate of 21.5 percentage points relative to receiving a neutral cue ($p<0.001$). On the other hand, an uncongenial cue leads to a lower correct response of 29.8 percentage points ($p<0.001$).
We also estimate how the cue that encourages guessing affects the ``Don't Know'' response rate. Presumably, a cue that encourages guessing would lead to a lower response rate for Don't Know. We find that the guessing cues do not have a very different effect from cues that do not. 

Overall, we find again using the YouGov survey and Texas Lyceum survey that questionnaire artifacts, via the addition of partisan cues in the same questions, affects the measured gaps in political knowledge. 


\section*{Discussion and Conclusion}

Since at least the publication of \cite{bartels_2002}, the conventional wisdom has been that partisan gaps in beliefs about politically consequential facts are wide and pervasive. 
The conventional wisdom in academia has also become the received wisdom for the mass public --- nearly 80\% of Americans believe that Democrats and Republicans  disagree on facts \citep{pew2018disagree}.

In line with other research on this topic \citep{bullocketal_2015, prior2015you, schaffner_luks} (though see \cite{berinsky_2017} and \cite{peterson_iyengar_forth}), our results suggests that a big chunk of partisan gap is not founded in differences in beliefs. We find that conventional aspects of survey items like not asking don't know, inserting a partisan cue, and treating inconfident answers as knowledge inflate the patisan gaps that we see on surveys. 

The fact that partisan gaps are smaller may seem at odds with some political behavior research but a careful reading of recent research suggests that small partisan gaps are to be expected. For instance, at odds with the theory of selective exposure, which posits vast imbalances in consumption of partisan news, recent studies show that most people consume very little political news \citep{Prior2007,flaxmanetal_2016}, and the news that they do consume is relatively balanced \citep{flaxmanetal_2016,garzetal_2018,gentzkowshapiro_2011,guess_2020}. Other evidence points to the fact that Democrats and Republicans update in light of events in a similar fashion \citep{gerber_annual_review,kernell_2019}.

The results however paint a mixed picture about democratic competence. Small gaps are partly a consequence of the fact that the average respondent doesn't have any confidently held beliefs about the issue at hand. It is mostly ignorance masquerading as partisan gaps. The upside is that partisan gaps are small and the downside is that people know even less than we assume.

\clearpage
\bibliographystyle{apsr}
\bibliography{pgap}

\clearpage

\input{./sections/appendix.tex}


\end{document}